{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal policy with Reinforcement Learning in Yahtzee dice game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class YahtzeeEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(YahtzeeEnv, self).__init__()\n",
    "        # Define spaces ( )\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=50, shape=(45,),\n",
    "                                                dtype=np.int32)  # ((dice_result: 5 * 6 bit for one hot encoding), reroll, turn, [scores: 12], bonus )\n",
    "        self.action_space = gym.spaces.Discrete(44)  # 0 : initial roll, 1-31: reroll, 32-43: filling the score)\n",
    "        self._reset()\n",
    "        self.rng = np.random.default_rng()\n",
    "\n",
    "    def _reset(self):\n",
    "        # Reset environment state (dice, categories, etc.)\n",
    "        # Game states\n",
    "        self.dice = np.zeros((5, 6), dtype=np.int32)  # one hot vectors in array\n",
    "        self.rerolls = 3\n",
    "        self.turn = 0\n",
    "        self.scorecard = np.zeros(12, dtype=np.int32)\n",
    "        self.bonus = False\n",
    "        self.done = False\n",
    "        self.bonusRewarded = False\n",
    "        self.scored = np.full((12,), False, dtype=np.bool)\n",
    "\n",
    "        # Return initial observation -> do we need this?\n",
    "\n",
    "    def _reroll_under_mask(self, mask: list):\n",
    "        \"Reroll dice result under the bitmask\"\n",
    "        if self.rerolls == 0:\n",
    "            raise ValueError(f\"No reroll remains. \")\n",
    "        else:\n",
    "            self.rerolls -= 1\n",
    "            for i in range(len(mask)):\n",
    "                if mask[i] == 1:\n",
    "                    self.dice[i, :] = np.zeros((1, 6), dtype=np.int32)  # reset ith die value\n",
    "                    j = self.rng.integers(low=0, high=5, endpoint=True, dtype=np.int32)\n",
    "                    self.dice[i, j] = 1\n",
    "\n",
    "    def get_score_for_action(self, action) -> int:\n",
    "        \"\"\" self.dice: 2D numpy array (5*6), each row represents one number under one-hot encoding\n",
    "            action : 31-42 integer number\n",
    "            Return : score(int) for selected action\n",
    "        \"\"\"\n",
    "        scoreto = action - 32  # Changed from 31 to 32\n",
    "        numbers = [0, 0, 0, 0, 0, 0]  # how many occurences are there for 1,2,...,5,6?\n",
    "        meresum = 0\n",
    "        for i in range(5):\n",
    "            if self.dice[i][0] == 1:  ## ith die is 1\n",
    "                numbers[0] += 1\n",
    "            elif self.dice[i][1] == 1:  ## ith die is 2\n",
    "                numbers[1] += 1\n",
    "            elif self.dice[i][2] == 1:\n",
    "                numbers[2] += 1\n",
    "            elif self.dice[i][3] == 1:\n",
    "                numbers[3] += 1\n",
    "            elif self.dice[i][4] == 1:\n",
    "                numbers[4] += 1\n",
    "            elif self.dice[i][5] == 1:\n",
    "                numbers[5] += 1\n",
    "            else:\n",
    "                continue\n",
    "        for i in range(5):\n",
    "            meresum += numbers[i] * (i + 1)\n",
    "\n",
    "        if scoreto == 0:  # Ones\n",
    "            return numbers[0] * 1\n",
    "        elif scoreto == 1:  # Twos\n",
    "            return numbers[1] * 2\n",
    "        elif scoreto == 2:  # Threes\n",
    "            return numbers[2] * 3\n",
    "        elif scoreto == 3:  # Fours\n",
    "            return numbers[3] * 4\n",
    "        elif scoreto == 4:  # Fives\n",
    "            return numbers[4] * 5\n",
    "        elif scoreto == 5:  # Sixes\n",
    "            return numbers[5] * 6\n",
    "        elif scoreto == 6:  # Choice\n",
    "            return meresum\n",
    "        elif scoreto == 7:  # Four-of-a-Kind: if any number appears at least 4 times\n",
    "            for i in range(5):\n",
    "                if numbers[i] >= 4:\n",
    "                    return meresum\n",
    "                else:\n",
    "                    continue\n",
    "            return 0\n",
    "        elif scoreto == 8:  # Full House: three of one number and two of another\n",
    "            return meresum if (3 in numbers) and (2 in numbers) else 0\n",
    "        elif scoreto == 9:  # Little Straight: 1,2,3,4; 2,3,4,5; 3,4,5,6\n",
    "\n",
    "            if numbers[0:4] == [1, 1, 1, 1]:\n",
    "                return 15\n",
    "            elif numbers[1:5] == [1, 1, 1, 1]:\n",
    "                return 15\n",
    "            elif numbers[2:] == [1, 1, 1, 1]:\n",
    "                return 15\n",
    "            else:\n",
    "                return 0\n",
    "        elif scoreto == 10:  # Big Straight: 2-3-4-5-6; 1,2,3,4,5\n",
    "            if (numbers[:5] == [1, 1, 1, 1, 1]) or (numbers[1:6] == [1, 1, 1, 1, 1]):\n",
    "                return 30\n",
    "            else:\n",
    "                return 0\n",
    "        elif scoreto == 11:  # Yacht: all dice the same\n",
    "            try:\n",
    "                numbers.index(5)\n",
    "                return 50\n",
    "            except ValueError:\n",
    "                return 0\n",
    "        else:\n",
    "            print(\"Undealt case raised. scoring function must be modified.\")\n",
    "            return 0\n",
    "\n",
    "    def _score_action(self, action, score=None):\n",
    "        \"\"\"Fill the score in scorecard with selected action. (in-place)\n",
    "         This function does\n",
    "         1) fill the score\n",
    "         2) reset rerolls to 3\n",
    "         3) turn increases\n",
    "         4) check game end\n",
    "         5) check bonus point is possible\n",
    "\n",
    "\n",
    "        Args:\n",
    "            action (int): 32-43 integer.\n",
    "            score (int, optional) : score for that category\n",
    "        No return value\n",
    "        \"\"\"\n",
    "        if action < 32 or action > 43:\n",
    "            raise ValueError(f\"Invalid scoring action: {action}. Action must be between 32 and 43 for scoring.\")\n",
    "\n",
    "        index = action - 32  # Changed from 31 to 32\n",
    "        if self.scored[index]:\n",
    "            raise ValueError(f\"Already filled in that category : {index}, value : {self.scorecard[index]}\")\n",
    "        val = score if score is not None else self.get_score_for_action(action)\n",
    "        self.scorecard[index] = val\n",
    "        self.scored[index] = True\n",
    "        self.rerolls = 3\n",
    "        self.turn += 1\n",
    "\n",
    "        # check if game is completed\n",
    "        if self.turn == 12:  # game end\n",
    "            self.done = True\n",
    "        # check the bonus point is possible\n",
    "        if np.sum(self.scorecard[0:6]) >= 63:\n",
    "            self.bonus = True\n",
    "\n",
    "    def _initiate_turn(self):\n",
    "        if self.rerolls == 3:\n",
    "            self._reroll_under_mask([1, 1, 1, 1, 1])\n",
    "\n",
    "    def get_state(self) -> np.ndarray:\n",
    "        return np.concatenate([\n",
    "            self.dice.flatten(),\n",
    "            np.array([self.rerolls]),  # Convert scalar to array\n",
    "            np.array([self.turn]),  # Convert scalar to array\n",
    "            self.scorecard,\n",
    "            np.array([self.bonus], dtype=int)  # Convert boolean to int\n",
    "        ])\n",
    "\n",
    "    def get_valid_action(self) -> list:\n",
    "        \"\"\"Return list of valid action, e.g.)[1,4,5,43] action: 0(initiate roll), 1-31(reroll), 32-43(scoring); integer \"\"\"\n",
    "\n",
    "        if self.rerolls == 3:\n",
    "            return [0] #only initial roll\n",
    "        elif self.rerolls == 0:\n",
    "            # only scoring option is available\n",
    "            valids = []\n",
    "            for i, filled in enumerate(self.scored):\n",
    "                if not filled:\n",
    "                    valids.append(i + 32)  # Changed from 31 to 32\n",
    "            return valids\n",
    "        else:\n",
    "            # We have rerolls, and scoring is available as well\n",
    "            valids = list(range(1, 32))  # Changed from 0 to 1\n",
    "            for i, filled in enumerate(self.scored):\n",
    "                if not filled:\n",
    "                    valids.append(i + 32)  # Changed from 31 to 32\n",
    "            return valids\n",
    "\n",
    "    def step(self, action):\n",
    "        # 1) Apply action (roll dice or choose category, etc.)\n",
    "        # 2) Calculate reward\n",
    "        # 3) Determine if episode is done\n",
    "        # 4) Return the next state, reward, done, and optionally info dict\n",
    "\n",
    "        valid = self.get_valid_action()\n",
    "        # print(f\"***Valid actions in step function(output of get_valid_function) : {valid}***\")\n",
    "\n",
    "        if action not in valid:\n",
    "            # If an invalid action is selected, penalize and return the same state.\n",
    "            reward = -15\n",
    "            next_state = self.get_state()\n",
    "            return next_state, reward, self.done, {}\n",
    "\n",
    "        ## turn initiation\n",
    "        if action == 0:\n",
    "            reward = 0\n",
    "            self._initiate_turn()\n",
    "            next_state = self.get_state()\n",
    "            return next_state, reward, self.done, {}\n",
    "\n",
    "        ## reroll action\n",
    "        if 1 <= action <= 31:  # Changed from < 31 to range 1-31\n",
    "            mask = self.int_to_bitmask(action)\n",
    "            self._reroll_under_mask(mask)\n",
    "            reward = self.get_sum_possible_score() * 0.08  # since this is not actual reward (not finally scored value)\n",
    "            next_state = self.get_state()\n",
    "            return next_state, reward, self.done, {}\n",
    "\n",
    "        ## scoring action\n",
    "        elif 32 <= action <= 43:  # Changed to range 32-43\n",
    "            score = self.get_score_for_action(action)\n",
    "            reward = score \n",
    "            self._score_action(action, score)\n",
    "            if self.bonus and (self.bonusRewarded is False):\n",
    "                reward += 35\n",
    "                self.bonusRewarded = True\n",
    "            # reward += score / 63  # bonus contribution\n",
    "            next_state = self.get_state()\n",
    "            return next_state, reward, self.done, {}\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid action: {action}. Action must be between 0 and 43.\")\n",
    "\n",
    "    def get_sum_possible_score(self) -> int:\n",
    "        valids = self.get_valid_action()\n",
    "        sum = 0\n",
    "        for sAction in valids:\n",
    "            if sAction >= 32:\n",
    "                sum += self.get_score_for_action(sAction)\n",
    "        return sum\n",
    "\n",
    "    @staticmethod\n",
    "    def int_to_bitmask(num):\n",
    "        \"\"\"Change an integer number to a 5-bit mask corresponding to (num+1) in binary representation.\n",
    "\n",
    "        Input: integer number (1-31)\n",
    "        Output: list of integers representing a 5-bit mask.\n",
    "\n",
    "        Example:\n",
    "            input: 21\n",
    "            return: [1, 0, 1, 1, 0] \n",
    "        \"\"\"\n",
    "        if not (1 <= num <= 31):\n",
    "            raise ValueError(\"Input number must be between 0(inclusive) and 30 (inclusive).\")\n",
    "\n",
    "        # Convert to binary, remove '0b' prefix, and fill with leading zeros to ensure 5 bits\n",
    "        bitmask = list(map(int, format(num, '05b'))) # change the num+1 to num\n",
    "        return bitmask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "\n",
    "class DQNet(nn.Module):\n",
    "    def __init__(self, state_dim=45, action_dim=44):\n",
    "        super(DQNet, self).__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Agent definition (optimizer, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99,\n",
    "                 epsilon_start=1.0, epsilon_end=0.010, epsilon_decay=0.995,\n",
    "                 buffer_size=10000, batch_size=64, target_update=100, rng=None):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update = target_update\n",
    "        self.steps_done = 0\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        #Cuda device\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.policy_net = DQNet(state_dim, action_dim).to(self.device)\n",
    "        self.target_net = DQNet(state_dim, action_dim).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        #self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.optimizer = optim.RMSprop(self.policy_net.parameters(),lr = lr, alpha = 0.99)\n",
    "        self.rng = np.random.default_rng() if rng is None else rng\n",
    "        \n",
    "            \n",
    "    def select_action(self, state, valid_actions):\n",
    "        \"\"\"\n",
    "        Choose an action using an epsilon-greedy strategy,\n",
    "        but only among the valid actions provided.\n",
    "        \"\"\"\n",
    "        self.steps_done += 1\n",
    "        # 1) With probability epsilon, pick a random valid action.\n",
    "        if self.rng.random() < self.epsilon:\n",
    "            return self.rng.choice(valid_actions)\n",
    "        else:\n",
    "            # 2) Otherwise, pick the best valid action based on Q-values from the policy_net.\n",
    "            with torch.no_grad():\n",
    "                # Move input state to the same device as the network.\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "                \n",
    "                # Forward pass (no_grad context, so no gradient tracking).\n",
    "                q_values = self.policy_net(state_tensor).squeeze(0)  # Shape: (action_dim,)\n",
    "\n",
    "                # Create a mask where all actions are -inf except valid ones.\n",
    "                masked_q = torch.full((self.action_dim,), float('-inf'), device=self.device)\n",
    "                masked_q[valid_actions] = q_values[valid_actions]\n",
    "\n",
    "                # Argmax on GPU; then convert to a Python int.\n",
    "                action = int(torch.argmax(masked_q))\n",
    "                if action not in valid_actions:\n",
    "                    print(f\"Invalid actions are selected. valid actions : {valid_actions}, action : {action}\")\n",
    "            \n",
    "            return action\n",
    "\n",
    "\n",
    "    def push_memory(self, transition):\n",
    "        # Transition: (state, action, reward, next_state, done, valid_actions_next)\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        # Sample a batch of transitions from memory\n",
    "        indices = self.rng.choice(len(self.memory), self.batch_size, replace=False)\n",
    "        transitions = [self.memory[i] for i in indices]\n",
    "\n",
    "        # transitions is a list of tuples:\n",
    "        # (state, action, reward, next_state, done, valid_actions_next)\n",
    "\n",
    "        # Convert lists to NumPy arrays\n",
    "        states = np.array([t[0] for t in transitions], dtype=np.float32)\n",
    "        actions = np.array([t[1] for t in transitions], dtype=np.int64)\n",
    "        rewards = np.array([t[2] for t in transitions], dtype=np.float32)\n",
    "        next_states = np.array([t[3] for t in transitions], dtype=np.float32)\n",
    "        dones = np.array([t[4] for t in transitions], dtype=np.float32)\n",
    "        valid_actions_next = [t[5] for t in transitions]\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        batch_state = torch.from_numpy(states).to(self.device)\n",
    "        batch_action = torch.from_numpy(actions).unsqueeze(1).to(self.device)\n",
    "        batch_reward = torch.from_numpy(rewards).unsqueeze(1).to(self.device)\n",
    "        batch_next_state = torch.from_numpy(next_states).to(self.device)\n",
    "        batch_done = torch.from_numpy(dones).unsqueeze(1).to(self.device)\n",
    "\n",
    "        # 1) Current Q-values for the taken actions (policy net)\n",
    "        q_values_all = self.policy_net(batch_state)            # [batch_size, action_dim]\n",
    "        current_q = q_values_all.gather(1, batch_action)       # [batch_size, 1]\n",
    "\n",
    "        # 2) Double DQN logic:\n",
    "        #    - Use policy_net to pick best action in the next state\n",
    "        #    - Then evaluate that action with target_net\n",
    "        with torch.no_grad():\n",
    "            # a) Next state Q-values from policy_net (for action selection)\n",
    "            next_q_policy = self.policy_net(batch_next_state)  # [batch_size, action_dim]\n",
    "\n",
    "            # b) Next state Q-values from target_net (for value)\n",
    "            next_q_target = self.target_net(batch_next_state)  # [batch_size, action_dim]\n",
    "\n",
    "            # We'll store the Double DQN chosen action's Q-value in next_q_values\n",
    "            next_q_values = torch.zeros((self.batch_size, 1), device=self.device)\n",
    "\n",
    "            for i, valid_acts in enumerate(valid_actions_next):\n",
    "                if len(valid_acts) > 0:\n",
    "                    # i) pick best valid action from policy_net\n",
    "                    best_action_idx = torch.argmax(next_q_policy[i, valid_acts])\n",
    "                    best_action = valid_acts[best_action_idx]  # This is the actual action index\n",
    "                    \n",
    "                    # ii) evaluate Q-value for that action from target_net\n",
    "                    next_q_values[i] = next_q_target[i, best_action]\n",
    "                else:\n",
    "                    # If no valid actions, Q-value remains 0\n",
    "                    next_q_values[i] = 0.0\n",
    "\n",
    "        # 3) Bellman target:\n",
    "        #    target = reward + (1 - done) * gamma * Q_target(next_state, best_action)\n",
    "        #    where best_action is chosen by the policy_net\n",
    "        expected_q = batch_reward + (1 - batch_done) * self.gamma * next_q_values\n",
    "\n",
    "        # 4) Compute loss and update\n",
    "        loss = nn.MSELoss()(current_q, expected_q)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def update_target(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train process definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from DeepQNet import DQNet\n",
    "from DeepQNet import DQNAgent\n",
    "from YahtzeeEnv import YahtzeeEnv\n",
    "\n",
    "def train_agent(num_episodes=500, print_interval=10, load_filepath=None, save_filepath=None, lr=1e-3, gamma=0.99,\n",
    "                 epsilon_start=1.0, epsilon_end=0.010, epsilon_decay=0.995,\n",
    "                 buffer_size=10000, batch_size=64, target_update=100, rng=None):\n",
    "    \"\"\"\n",
    "    Train the DQN agent on YahtzeeEnv for a specified number of episodes.\n",
    "    Optionally load an existing agent's parameters from 'load_filepath'\n",
    "    and/or save the agent after training to 'save_filepath'.\n",
    "    \n",
    "    Args:\n",
    "        num_episodes (int): Number of episodes (full games) to train.\n",
    "        print_interval (int): Print progress every this many episodes.\n",
    "        load_filepath (str or None): Path to a saved agent checkpoint. \n",
    "                                     If provided, loads that agent first.\n",
    "        save_filepath (str or None): Path to save the trained agent after training.\n",
    "    \n",
    "    Returns:\n",
    "        DQNAgent: The trained (or further trained) DQN agent.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize environment.\n",
    "    env = YahtzeeEnv()\n",
    "    # The state dimension is determined from the environment.\n",
    "    state_dim = env.get_state().shape[0]\n",
    "    # As defined in YahtzeeEnv, there are 43 discrete actions (1..43).\n",
    "    action_dim = 44\n",
    "\n",
    "    # 1) Either load an existing agent or create a new one.\n",
    "    if load_filepath is not None:\n",
    "        # We load from a checkpoint file.\n",
    "        print(f\"Loading agent from {load_filepath}...\")\n",
    "        agent = load_agent(load_filepath, state_dim, action_dim)\n",
    "        agent.epsilon = epsilon_start\n",
    "    else:\n",
    "        # We create a new agent from scratch.\n",
    "        agent = DQNAgent(state_dim, action_dim, lr=lr, gamma=gamma,\n",
    "                 epsilon_start=epsilon_start, epsilon_end=epsilon_end, epsilon_decay=epsilon_decay,\n",
    "                 buffer_size=buffer_size, batch_size=batch_size, target_update=target_update, rng=rng)\n",
    "    \n",
    "    total_steps = 0  # Counts total steps across episodes (for target_net updates).\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        # Reset the environment for a new episode (a new Yahtzee game).\n",
    "        env._reset()\n",
    "        state = env.get_state()\n",
    "        \n",
    "        episode_reward = 0.0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Retrieve valid actions for the current state.\n",
    "            valid_actions = env.get_valid_action()\n",
    "            \n",
    "            # Agent picks an action (epsilon-greedy restricted to valid actions).\n",
    "            action = agent.select_action(state, valid_actions)\n",
    "            \n",
    "            # Environment processes the action.\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "\n",
    "            # If not done, get valid actions for the next state; otherwise empty list.\n",
    "            next_valid_actions = env.get_valid_action() if not done else []\n",
    "            \n",
    "            # Store transition in replay memory.\n",
    "            agent.push_memory((state, action, reward, next_state, done, next_valid_actions))\n",
    "            \n",
    "            # Optimize (update) the policy network using a minibatch from memory.\n",
    "            agent.optimize_model()\n",
    "            \n",
    "            # Move to the next state.\n",
    "            state = next_state\n",
    "            total_steps += 1\n",
    "            \n",
    "            # Periodically update the target network with policy_net weights.\n",
    "            if total_steps % agent.target_update == 0:\n",
    "                agent.update_target()\n",
    "        \n",
    "        #  Decay epsilon AFTER the episode ends, not after every batch update\n",
    "        try : \n",
    "            if agent.epsilon > agent.epsilon_end and (episode % (num_episodes // 600)==0):\n",
    "                agent.epsilon *= agent.epsilon_decay  \n",
    "        except ZeroDivisionError:\n",
    "            pass\n",
    "        # Print training progress every 'print_interval' episodes.\n",
    "        if (episode + 1) % print_interval == 0:\n",
    "            print(f\"Episode {episode+1}/{num_episodes} - Reward: {episode_reward:.2f}, Score: {env.scorecard[0:6]}|{env.scorecard[6:]},total : {np.sum(env.scorecard)}, Epsilon: {agent.epsilon:.3f}\")\n",
    "\n",
    "    # After training, optionally save the agent.\n",
    "    if save_filepath is not None:\n",
    "        save_agent(agent, save_filepath)\n",
    "\n",
    "    return agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_agent(agent, num_test_episodes=20):\n",
    "    \"\"\"\n",
    "    Test a trained DQN agent (no/low exploration) on YahtzeeEnv.\n",
    "    \n",
    "    Args:\n",
    "        agent (DQNAgent): A trained DQNAgent (with .policy_net on agent.device).\n",
    "        num_test_episodes (int): Number of episodes to test (full Yahtzee games).\n",
    "        \n",
    "    Returns:\n",
    "        float: The average reward over the test episodes.\n",
    "    \"\"\"\n",
    "    # Temporarily store the old epsilon, then set epsilon to 0 for pure exploitation.\n",
    "    old_epsilon = agent.epsilon\n",
    "    agent.epsilon = 0.0  # No exploration during testing\n",
    "\n",
    "    # We'll move input data to the same device as agent.policy_net\n",
    "    device = agent.device\n",
    "\n",
    "    env = YahtzeeEnv()\n",
    "    rewards = []\n",
    "\n",
    "    for episode in range(num_test_episodes):\n",
    "        env._reset()\n",
    "        state = env.get_state()\n",
    "        episode_reward = 0.0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            valid_actions = env.get_valid_action()\n",
    "            \n",
    "            # Inference on GPU (or CPU if CUDA not available)\n",
    "            with torch.no_grad():\n",
    "                # Move the state to the same device as the model\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "                \n",
    "                # Forward pass on the policy network\n",
    "                q_values_tensor = agent.policy_net(state_tensor).squeeze(0)  \n",
    "                \n",
    "                # Move back to CPU for NumPy-based masking\n",
    "                q_values = q_values_tensor.cpu().numpy()\n",
    "            \n",
    "            # Create a masked Q-value array that sets invalid actions to -∞\n",
    "            masked_q_values = np.full(agent.action_dim, -np.inf)\n",
    "            for a in valid_actions:\n",
    "                masked_q_values[a] = q_values[a]\n",
    "            \n",
    "            # Choose the best action among valid actions (greedy)\n",
    "            best_action = int(np.argmax(masked_q_values))\n",
    "            \n",
    "            # Step in the environment (on CPU)\n",
    "            next_state, reward, done, _ = env.step(best_action)\n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Move on to the next state\n",
    "            state = next_state\n",
    "\n",
    "        rewards.append(episode_reward)\n",
    "\n",
    "    # Restore agent’s original epsilon\n",
    "    agent.epsilon = old_epsilon\n",
    "\n",
    "    # Compute average reward\n",
    "    avg_reward = np.mean(rewards)\n",
    "    print(f\"Tested on {num_test_episodes} episodes. Avg reward = {avg_reward:.2f}\")\n",
    "    return avg_reward\n",
    "\n",
    "\n",
    "def save_agent(agent, filepath=\"trained_agent.pth\"):\n",
    "    \"\"\"\n",
    "    Save the trained DQN agent's policy network and parameters.\n",
    "\n",
    "    Args:\n",
    "        agent (DQNAgent): The trained agent to save.\n",
    "        filepath (str): Path to save the model.\n",
    "    \"\"\"\n",
    "    torch.save({\n",
    "        'policy_net_state_dict': agent.policy_net.state_dict(),\n",
    "        'target_net_state_dict': agent.target_net.state_dict(),\n",
    "        'optimizer_state_dict': agent.optimizer.state_dict(),\n",
    "        'epsilon': agent.epsilon\n",
    "    }, filepath)\n",
    "    print(f\"Agent saved to {filepath}\")\n",
    "\n",
    "def save_model_info(model, optimizer, loss_fn, filename=\"modelinfo.md\"):\n",
    "    with open(filename, \"w\") as file:\n",
    "        file.write(\"# Model Information\\n\\n\")\n",
    "        \n",
    "        # 모델의 layer 구조\n",
    "        file.write(\"## Model Architecture\\n\")\n",
    "        file.write(\"```python\\n\")\n",
    "        file.write(str(model) + \"\\n\")\n",
    "        file.write(\"```\\n\\n\")\n",
    "        \n",
    "        # Optimizer 정보\n",
    "        file.write(\"## Optimizer\\n\")\n",
    "        file.write(f\"Optimizer: {optimizer.__class__.__name__}\\n\")\n",
    "        file.write(f\"Learning rate: {optimizer.param_groups[0]['lr']}\\n\")\n",
    "        file.write(f\"Parameters: {optimizer.param_groups[0]}\\n\\n\")\n",
    "        \n",
    "        # Loss function 정보\n",
    "        file.write(\"## Loss Function\\n\")\n",
    "        file.write(f\"Loss Function: {loss_fn.__class__.__name__}\\n\\n\")\n",
    "        \n",
    "        # Activation functions 정보 (모델에서 사용하는 activation function 추출)\n",
    "        file.write(\"## Activation Functions\\n\")\n",
    "        activation_functions = []\n",
    "        for layer in model.children():\n",
    "            if isinstance(layer, nn.ReLU):\n",
    "                activation_functions.append(\"ReLU\")\n",
    "            elif isinstance(layer, nn.Sigmoid):\n",
    "                activation_functions.append(\"Sigmoid\")\n",
    "            elif isinstance(layer, nn.Tanh):\n",
    "                activation_functions.append(\"Tanh\")\n",
    "        \n",
    "        if activation_functions:\n",
    "            file.write(f\"Used activation functions: {', '.join(activation_functions)}\\n\")\n",
    "        else:\n",
    "            file.write(\"No activation function found\\n\")\n",
    "\n",
    "def load_agent(filepath, state_dim, action_dim):\n",
    "    \"\"\"\n",
    "    Load a trained DQN agent from a file.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Path to the saved model checkpoint.\n",
    "        state_dim (int): State dimension (should match training).\n",
    "        action_dim (int): Action dimension (should match training).\n",
    "\n",
    "    Returns:\n",
    "        DQNAgent: The loaded agent.\n",
    "    \"\"\"\n",
    "    # Create a new agent instance with the same architecture\n",
    "    agent = DQNAgent(state_dim, action_dim)\n",
    "\n",
    "    # Load the checkpoint file\n",
    "    checkpoint = torch.load(filepath, map_location=torch.device('cpu'))  # Use CPU for portability\n",
    "    \n",
    "    # Restore model parameters\n",
    "    agent.policy_net.load_state_dict(checkpoint['policy_net_state_dict'])\n",
    "    agent.target_net.load_state_dict(checkpoint['target_net_state_dict'])\n",
    "    agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    agent.epsilon = checkpoint['epsilon']\n",
    "    \n",
    "    print(f\"Agent loaded from {filepath}\")\n",
    "    return agent\n",
    "\n",
    "def find_latest_trial(num_episodes):\n",
    "    \"\"\"\n",
    "    Find the largest trial number among files in the current directory\n",
    "    matching the pattern: trained_agent_{trial}_{num_episodes}.pth\n",
    "\n",
    "    Returns:\n",
    "        int: the largest trial found, or -1 if no matching file exists.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r\"^trained_agent_(\\d+)_(\\d+)\\.pth$\")\n",
    "    max_trial = -1\n",
    "\n",
    "    # Check every file in the current working directory\n",
    "    for fname in os.listdir('.'):\n",
    "        match = pattern.match(fname)\n",
    "        if match:\n",
    "            found_trial = int(match.group(1))\n",
    "            found_episodes = int(match.group(2))\n",
    "            if found_episodes == num_episodes and found_trial > max_trial:\n",
    "                max_trial = found_trial\n",
    "\n",
    "    return max_trial\n",
    "\n",
    "def play_episode(agent, md_filename=\"yahtzee_playthrough.md\"):\n",
    "    \"\"\"\n",
    "    Plays one episode of Yahtzee with a trained agent, writing each step's info to a Markdown file.\n",
    "    \"\"\"\n",
    "    env = YahtzeeEnv()\n",
    "    env._reset()\n",
    "    done = False\n",
    "    steps = 0\n",
    "    cumulative_reward = 0\n",
    "    category = {\n",
    "    0: \"initiate roll\",  # 처음 굴리기\n",
    "\n",
    "    # 주사위를 다시 굴리는 행동\n",
    "    1:  \"reroll 00001\",  2:  \"reroll 00010\",  3:  \"reroll 00011\",  4:  \"reroll 00100\",\n",
    "    5:  \"reroll 00101\",  6:  \"reroll 00110\",  7:  \"reroll 00111\",  8:  \"reroll 01000\",\n",
    "    9:  \"reroll 01001\", 10:  \"reroll 01010\", 11: \"reroll 01011\", 12: \"reroll 01100\",\n",
    "    13: \"reroll 01101\", 14: \"reroll 01110\", 15: \"reroll 01111\", 16: \"reroll 10000\",\n",
    "    17: \"reroll 10001\", 18: \"reroll 10010\", 19: \"reroll 10011\", 20: \"reroll 10100\",\n",
    "    21: \"reroll 10101\", 22: \"reroll 10110\", 23: \"reroll 10111\", 24: \"reroll 11000\",\n",
    "    25: \"reroll 11001\", 26: \"reroll 11010\", 27: \"reroll 11011\", 28: \"reroll 11100\",\n",
    "    29: \"reroll 11101\", 30: \"reroll 11110\", 31: \"reroll 11111\",\n",
    "\n",
    "    # 점수를 기록하는 행동\n",
    "    32: \"score : ones\", 33: \"score : twos\", 34: \"score : threes\", 35: \"score : fours\",\n",
    "    36: \"score : fives\", 37: \"score : sixes\", 38: \"score : choices\", 39: \"score : four of a kind\",\n",
    "    40: \"score : full house\", 41: \"score : small straight\", 42: \"score : large straight\",\n",
    "    43: \"score : yahtzee\",\n",
    "}\n",
    "\n",
    "\n",
    "    # We prepare lines of markdown\n",
    "    md_lines = []\n",
    "    md_lines.append(\"# Yahtzee Episode Playthrough\\n\")\n",
    "    md_lines.append(\"**Environment:** Yahtzee\\n\")\n",
    "    md_lines.append(\"**Agent:** Trained DQN (placeholder)\\n\")\n",
    "    md_lines.append(\"---\\n\")\n",
    "\n",
    "    md_lines.append(\"## Step-by-Step Decisions\\n\")\n",
    "    md_lines.append(\"| Step | Dice (One-Hot) | Rerolls | Turn | Valid Actions | Chosen Action | Reward | Cumulative Reward | Done? |\")\n",
    "    md_lines.append(\"| --- | --- | --- | --- | --- | --- | --- | --- | --- |\")\n",
    "\n",
    "    # Start the loop\n",
    "    state = env.get_state()\n",
    "    while not done and steps < 100:  # 12 turns is typical, 100 is a safe upper bound\n",
    "        valid_actions = env.get_valid_action()\n",
    "        action = agent.select_action(state, valid_actions)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        steps += 1\n",
    "        cumulative_reward += reward\n",
    "\n",
    "        # Convert dice to a more readable list of faces\n",
    "        dice_desc = []\n",
    "        for i in range(5):\n",
    "            face_idx = np.argmax(env.dice[i])  # which face is \"1\"\n",
    "            dice_desc.append(str(face_idx+1))\n",
    "        dice_str = \", \".join(dice_desc)\n",
    "\n",
    "        # Summarize step in table row\n",
    "        line = f\"| {steps} | **{dice_str}** | {next_state[30]} | {next_state[31]} | `{next_state[32:43].astype(int)}` | **{category[action]}** | {reward:.2f} | {cumulative_reward:.2f} | {done} |\"\n",
    "        md_lines.append(line)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    # Summarize final score\n",
    "    final_score = np.sum(env.scorecard)\n",
    "    bonus_desc = f\"(Bonus Active)\" if env.bonus else \"\"\n",
    "    md_lines.append(\"\\n---\\n\")\n",
    "    md_lines.append(f\"**Episode finished** after **{steps}** steps.\\n\\n\")\n",
    "    md_lines.append(f\"**Final Scorecard** = {env.scorecard}  \\n\")\n",
    "    md_lines.append(f\"**Sum of Scorecard** = {final_score} {bonus_desc}\\n\")\n",
    "    md_lines.append(f\"**Cumulative Reward** = {cumulative_reward:.2f}\\n\")\n",
    "\n",
    "    # Write to Markdown file\n",
    "    with open(md_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(md_lines))\n",
    "\n",
    "    print(f\"Playthrough complete. Markdown log written to {md_filename}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Just a debug check: prints True if GPU is available\n",
    "    print(\"CUDA available?\", torch.cuda.is_available())\n",
    "    #print(\"Device:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    torch.device(\"cpu\")\n",
    "\n",
    "    # Number of training episodes for this run\n",
    "    num_episodes = 1000\n",
    "\n",
    "    # 1) Look for an existing trial file in the current directory\n",
    "    trial = find_latest_trial(num_episodes)\n",
    "    if trial >= 0:\n",
    "        # Found a file trained_agent_{trial}_{num_episodes}.pth\n",
    "        load_filepath = f\"trained_agent_{trial}_{num_episodes}.pth\"\n",
    "        print(f\"Loading from file: {load_filepath}\")\n",
    "    else:\n",
    "        load_filepath = None  # No prior file found\n",
    "        print(\"No prior training file found; starting from scratch.\")\n",
    "\n",
    "    # 2) Define where to save next trial\n",
    "    #    e.g. if trial = 2, we save next as trained_agent_3_{num_episodes}.pth\n",
    "    save_filepath = f\"trained_agent_{trial+1}_{num_episodes}.pth\"\n",
    "\n",
    "    # 3) Train (or continue training) the agent and save the model to save_filepath\n",
    "    rng = np.random.default_rng()\n",
    "    trained_agent = train_agent(\n",
    "        num_episodes=num_episodes,\n",
    "        print_interval=50,\n",
    "        load_filepath=load_filepath,      # Might be None if not found\n",
    "        save_filepath=save_filepath, lr=5e-5, gamma=0.99,\n",
    "                 epsilon_start=1.0 / (trial+2), epsilon_end=0.010, epsilon_decay=0.995,\n",
    "                 buffer_size=100000, batch_size=128, target_update=100, rng=rng\n",
    "    )\n",
    "\n",
    "    # 4) Test the agent\n",
    "    test_score = test_agent(trained_agent, num_test_episodes=100)\n",
    "    print(f\"Final average test reward: {test_score:.2f}\")\n",
    "    play_episode(trained_agent)\n",
    "\n",
    "    # 5) save the model info in modelinfo.md\n",
    "    save_model_info(trained_agent.policy_net.net, trained_agent.optimizer, nn.MSELoss())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
